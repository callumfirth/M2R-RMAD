@article{dhamarticle,
author = {Gebremedhin, Assefaw H. and Walther, Andrea},
title = {An introduction to algorithmic differentiation},
journal = {WIREs Data Mining and Knowledge Discovery},
volume = {10},
number = {1},
keywords = {adjoints, algorithmic differentiation, automatic differentiation, backpropagation, checkpointing, sensitivities},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1334},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1334},
abstract = {Abstract Algorithmic differentiation (AD), also known as automatic differentiation, is a technology for accurate and efficient evaluation of derivatives of a function given as a computer model. The evaluations of such models are essential building blocks in numerous scientific computing and data analysis applications, including optimization, parameter identification, sensitivity analysis, uncertainty quantification, nonlinear equation solving, and integration of differential equations. We provide an introduction to AD and present its basic ideas and techniques, some of its most important results, the implementation paradigms it relies on, the connection it has to other domains including machine learning and parallel computing, and a few of the major open problems in the area. Topics we discuss include: forward mode and reverse mode of AD, higher-order derivatives, operator overloading and source transformation, sparsity exploitation, checkpointing, cross-country mode, and differentiating iterative processes. This article is categorized under: Algorithmic Development > Scalable Statistical Methods Technologies > Data Preprocessing},
year = {2020}
}

@book{PoPBook,
author = {Ham, David},
title = {Object-oriented Programming in Python for Mathematicians},
url = {https://object-oriented-python.github.io/},
year = {2021}
}

@book{quarteroni,
  title={Numerical Mathematics},
  author={Quarteroni, A. and Sacco, R. and Saleri, F.},
  series={Texts in Applied Mathematics},
  year={2007},
  publisher={Springer}
}

@article{chem,
title = {On computational differentiation},
journal = {Computers & Chemical Engineering},
volume = {22},
number = {4},
pages = {475-490},
year = {1998},
url = {https://www.sciencedirect.com/science/article/pii/S0098135497002640},
author = {John E. Tolsma and Paul I. Barton},
abstract = {Numerical derivatives play an important role in many computations. In many applications, the cost associated with evaluation of numerical derivatives may be significant. Dramatic improvements in the speed of such calculations can be obtained through careful consideration of how these derivatives are computed. This paper reviews several ways in which numerical derivatives can be evaluated: hand-coding, finite difference approximations, reverse polish notation evaluation, symbolic differentiation, and automatic differentiation. It is concluded that automatic differentiation has significant advantages over all other approaches. Several ways of improving the efficiency of obtaining derivatives in an interpretive, symbolic environment are discussed. Example problems are compared to illustrate these improvements.}
}

@article{introAD,
author = {Griewank, Andreas and Walther, Andrea},
title = {Introduction to Automatic Differentiation},
journal = {PAMM},
volume = {2},
number = {1},
pages = {45-49},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pamm.200310012},
abstract = {Abstract Automatic, or algorithmic, differentiation (AD) is a chain rule-based technique for evaluating derivatives of functions given as computer programs for their elimination. We review the main characteristics and application of AD and illustrate the methodology on a simple example.},
year = {2003}
}

@book{evald,
author = {Griewank, Andreas and Walther, Andrea},
title = {Evaluating Derivatives},
publisher = {Society for Industrial and Applied Mathematics},
year = {2008},
edition   = {Second},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898717761},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898717761}
}

@article{falisse,
  title={Algorithmic differentiation improves the computational efficiency of OpenSim-based trajectory optimization of human movement},
  author={Falisse, Antoine and Serrancol{\'\i}, Gil and Dembia, Christopher L and Gillis, Joris and De Groote, Friedl},
  journal={PLoS One},
  volume={14},
  number={10},
  pages={e0217730},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{nnad,
  title={Backwards differentiation in AD and neural nets: Past links and new opportunities},
  author={Werbos, Paul J},
  journal={Automatic differentiation: Applications, theory, and implementations},
  pages={15--34},
  year={2006},
  publisher={Springer}
}

@book{appad,
	author={Martin BÃ¼cker and George Corliss and Paul Hovland and Uwe Naumann and Boyana Norris},
	year={2006},
	title={Automatic Differentiation: Applications, Theory, and Implementations},
	publisher={Springer-Verlag},
	volume={50},
	edition={First Edition},
	abstract={This collection covers the state of the art in automatic differentiation theory and practice. Practitioners and students will learn about advances in automatic differentiation techniques and strategies for the implementation of robust and powerful tools. Computational scientists and engineers will benefit from the discussion of applications, which provide insight into effective strategies for using automatic differentiation for design optimization, sensitivity analysis, and uncertainty quantification. Written for:Computational scientists Keywords: automatic differentiation, optimization, sensitivity analysis.},
	url={http://ebooks.ciando.com/book/index.cfm/bok_id/14303}
}