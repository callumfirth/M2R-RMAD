\documentclass{article}

\usepackage{floatrow}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{parskip} % Required to stop indenting new paragraphs
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[numbers]{natbib} % number references and natbib for more styles
\bibliographystyle{unsrtnat} % sets bibliography to vancouver style

\title{Reverse Mode Algorithmic Differentiation}
\author{cf1021, sam221, aml21, mf621}
\date{}

\begin{document}

\maketitle
\tableofcontents
\newpage
\section{Introduction}

Possibly the broadest issue we have with applying mathematical theory on a computer is that computers can only work with discrete values and since the very essence of a derivative includes $\lim_{x \to 0}$ we immediately run into issues. Immediately when trying to solve this problem one considers first principles where we set a small value $\epsilon$ and then compute the equation:

\begin{equation*}
    \frac{f(x+\epsilon) - f(x)}{\epsilon}
\end{equation*}

This method is called the Finite Differences method. This can give us good approximations however has some drawbacks, the first of all being we have to set $\epsilon$, and an optimal value for this may change depending on our function and the value $x$ that we evaluate at and another being that we will never be exact and always have an error, sometimes many orders higher than machine epsilon. Another common method that is used is symbolic differentiation, which takes mathematical expressions and finds their derivatives from a set of rules much like you or I would do on paper. This method although very accurate is tricky to implement, especially with larger functions, and will slow on multivariate functions when trying to compute all the partial derivatives.

In this paper, we will be discussing algorithmic differentiation, this method breaks down a function into all its elementary operations and applies the chain rule to compute the derivative. Specifically, we will be looking at the reverse mode of algorithmic differentiation as it computes the derivates an order n faster than the forward mode, with n being the number of inputs to the function.

The use of derivatives in algorithms is increasingly clear as artificial intelligence develops and proves its own uses. This is because of backpropagation which is a method used to train neural nets. On these neural nets, one has many nodes connected by a series of weights, such that the change in one value affects the change in another, to optimise these weights we must use a method called gradient descent which first requires us to find a derivative.

Another use is in solving non-linear systems one method is Newton-Raphson in which iterates are calculated, given an initial $x_0$, using
\begin{equation}
\mathbf{x_{n+1}} = \mathbf{x_n} - [f'(\mathbf{x_n})]^{-1}f(\mathbf{x_n})
\end{equation}
where
\begin{equation}
    f'(\mathbf{x}) = \begin{bmatrix}
        \frac{\partial f_i}{\partial }
    \end{bmatrix}
\end{equation}
 
Hence the computation of derivatives of functions is key to finding the solutions of non-linear systems.

\section{Basics of Computing Derivatives}

There are three common ways to differentiate a function on a computer: numerically, symbolically, and algorithmically. The latter which we will be discussing in depth in this paper. Notably focusing on reverse mode algorithmic differentiation (AD). In contrast to symbolic and algorithmic differentiation, numerical differentiation results in an approximation for the derivative.

The most common example of numerical differentiation is the finite differences method. For \\ $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ a partial derivative can be approximated using a truncated Taylor series around $p \in \mathbb{R}^n$. From \cite{chem}
\begin{equation*}
    \frac{\partial f_i (p)}{\partial x_j} \approx \frac{f_i(p+he_j) - f_i(p)}{h}
\end{equation*}
This calculation for the derivative is approximate due to the error in the truncated Taylor series. The run time complexity of using finite differences to calculate all the partial derivatives of $f_i$ with respect to all $x_j$ is $\mathcal{O}(n)$ \cite{dhamarticle}. thus, finite differences is unsuited for systems with a large number of unknowns.

Symbolic differentiation involves taking an algebraic expression and computing its derivative with respect to a specified variable through repeated application of basic rules of calculus using computer algebra tools. It can be useful as, once the formula for the derivative is computed, finding the value of the derivative at a point is as simple as substituting it into the formula for the derivative.  Further, symbolic differentiation provides an explicit formula for the derivative which is beneficial from a mathematical point of view. However, it has the drawback that it can only be applied when the function to be differentiated exists in a closed form. Symbolic differentiation, like finite differences, is $\mathcal{O}(n)$ \cite{chem} hence it is inappropriate for large systems of unknowns.

There are two forms of algorithmic differentiation: forward mode and reverse mode. Fundamental to AD is the application of the chain rule. Both have similar computational costs but are suited to different types of functions. Suppose $F: \mathbb{R}^n \rightarrow \mathbb{R}^m$, $F(x_1, \cdots, x_n) = (y_1, \cdots, y_m)$ and we wish to compute the Jacobian of this function
\begin{equation*}
    J = F'(\Vec{x}) = \begin{bmatrix}
        \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
    \end{bmatrix} \in \mathbb{R}^{m \times n}
\end{equation*}
with forward mode algorithmic differentiation, this has computational cost $\mathcal{O}(n)$. In contrast, the computational cost with reverse mode algorithmic differentiation is $\mathcal{O}(m)$ \cite{falisse}. Hence, reverse mode is suited for functions $F$ with fewer output variables or when calculating 
\begin{equation*}
    \frac{\partial y_i}{\partial x_1}, \cdots, \frac{\partial y_i}{\partial x_n}
\end{equation*}
for a $i \in \{1, \dots, m \}$.
One drawback of reverse mode algorithmic differentiation over forward-mode is that it requires higher memory usage, due to the necessity to store intermediary values.

\section{Directed Acylic Graphs to Represent Expressions}

\begin{figure}[t]
    \centering
    \includegraphics[width=12cm]{images/Clustergraph.gv.pdf}
    \caption{A DAG and Expression Tree representation of Equation \ref{example1}}
    \label{fig:DAGgraph2}
\end{figure}

As in \cite{PoPBook}, we can represent an arithmetic expression as an expression tree. Here we represent each operator, symbol and number in an given expression as nodes in a graph, we denote these as elements. The operands of each element are the children of the node. Notation wise we will represent links out of a node indicating the children of the node, even though expression wise the children are "inputs" to such parent. We can also write an expression as a directed acylic graph which allows us to represent expressions with more than one operators to avoid recursion. An example of the difference between the two can be see in Figure \ref{fig:DAGgraph2}. We from now on will use DAG's to represent expressions as this is essential in lowering runtime and allowing fast computation of complicated expressions. We will presume that ordering of each child is important.

For a function $F: \mathbb{R}^3 \rightarrow \mathbb{R}^2$ defined by
\begin{equation} \label{example1}
    F \begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix} = \begin{pmatrix}
        \sin (x^2 y) + e^{x^2} \\ e^{x^2} \log z
    \end{pmatrix}
\end{equation}
the representation as a DAG and an expression tree can be seen in Figure \ref{fig:DAGgraph2}.

\section{Fundamentals of Algorithmic Differentiation}

Using notation from  \cite{evald}, let $F: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and $F(x) = y$ as in Section 2. Assume $F$ is the composition of a sequence of continuously differentiable elemental functions $(\varphi_i)_{i=1,\ldots, l}$, the derivatives of which can be easily calculated. Hence, $F$ can be represented as a DAG. In our case we will use the elemental functions provided below.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Function & Derivative \\
        \hline
        $\sin(u)$ & $\cos(u)$ \\
        $\cos(u)$ & $-\sin(u)$ \\
        $\exp(u) $ & $\exp(u)$ \\
        $\log(u)$ & $1/u$ \\
        $u^k$ & $k u^{k-1}$ \\
        $f(u)g(u)$ & $f'(u)g(u) + f(u)g'(u)$ \\
        $f(u) + g(u)$ & $f'(u) + g'(u)$ \\
        \hline
    \end{tabular}
    \caption{Derivatives of Elemental Functions}
    \label{tab:elemental}
\end{table}

The quantities calculated at each node of the graph during the function evaluation are numbered such that
\begin{equation*}
    [ \underbrace{v_{1-n}, \ldots, v_0}_{x} , v_1, \ldots, v_{l-m}, \underbrace{v_{l-m+1}, \ldots, v_l}_{y}]
\end{equation*}
Using $j \prec i$ to show $v_i$ depends directly on $v_j$ and $j \prec i \Longrightarrow j < i$ then it can be written
\begin{equation*}
    v_i = \varphi_i (v_j)_{j \prec i} \text{ for } \varphi_i : \mathbb{R}^{n_i} \longrightarrow \mathbb{R}
\end{equation*}

\subsection{Forward Mode Algorithmic Differentiation}

We will use the interpretation of forward mode AD given in \cite{dhamarticle}. Assume $x$ is equal to the value a time-dependent $x(t)$ takes at $t=0$. Then define the tangent 
\begin{equation*}
    \dot{x} = \frac{\partial x}{\partial t} \Big|_{t=0}
\end{equation*}
As $y = F(x)$ we calculate the tangent $\dot{y}$ as
\begin{equation*}
    \Dot{y} = \frac{\partial}{\partial t} F(x (t)) \Big|_{t=0} 
    = F'(x (t)) \frac{\partial x}{\partial t} \Big|_{t=0}
    = F'(x) \Dot{x}
    \equiv \Dot{F}(x, \Dot{x})
\end{equation*}
Using the additional notation $u_i = (v_j)_{j \prec i}$ and $\Dot{u}_i = (\Dot{v}_j)_{j \prec i}$, the idea can be applied to an elemental function $\varphi_i : \mathbb{R}^{n_i} \longrightarrow \mathbb{R}$, $v_i = \varphi_i (u_i)$ to get the result
\begin{equation}
    \label{tangentequ}
    \Dot{v_i} = \sum_{j \prec i} \left\{ \Dot{v}_j \cdot \frac{\partial}{\partial v_j} \varphi_i (u_i) \right\} 
    \equiv \Dot{\varphi}_i(u_i, \Dot{u}_i)
\end{equation}

This can be summarised as the general evaluation procedure given in \cite{evald} and given in Table \ref{tab:gep}.

\begin{table}[h]
    \centering
    \begin{tabular}{|lcll|}
        \hline
        $v_{i-n}$ & $\equiv$ & $x_i$ & $i = 1, \ldots, n$ \\
        $\Dot{v}_{i-n}$ & $\equiv$ & $\Dot{x}_i$ & $i = 1, \ldots, n$ \\
        \hline
        $v_{i}$ & $\equiv$ & $v_i = \varphi_i (u_i)$ & $i = 1, \ldots, l$ \\
        $\Dot{v}_{i}$ & $\equiv$ & $\sum_{j \prec i} \left\{ \Dot{v}_j \cdot \frac{\partial}{\partial v_j} \varphi_i (u_i) \right\}$ & $i = 1, \ldots, l$ \\
        \hline
        $v_{l-m+i}$ & $\equiv$ & $y_i$ & $i = 1, \ldots, m$ \\
        $\Dot{v}_{l-m+i}$ & $\equiv$ & $\Dot{y}_i$ & $i = 1, \ldots, m$ \\
        \hline
    \end{tabular}
    \caption{General Evaluation Procedure}
    \label{tab:gep}
\end{table}

When represented on a DAG, forward mode AD begins at the inputs and moves through the graph computing $F$ and $\Dot{F}$ together at each node. For this a concise version of Table \ref{tab:gep} is provided in Table \ref{tab:gtp}.

\begin{table}[h]
    \centering
    \begin{tabular}{|lcll|}
        \hline
        $[v_{i-n}, \Dot{v}_{i-n}]$ & $=$ & $[x_{i}, \Dot{x}_{i}]$ & $i = 1, \ldots, n$ \\
        \hline
        $[v_{i}, \Dot{v}_{i}]$ & $=$ & $[\varphi_i (u_i), \Dot{\varphi}_i(u_i, \Dot{u}_i)]$ & $i = 1, \ldots, l$ \\
        \hline
        $[v_{l-m+1}, \Dot{v}_{l-m+1}]$ & $=$ & $[y_{i}, \Dot{y}_{i}]$ & $i = 1, \ldots, m$ \\
        \hline
    \end{tabular}
    \caption{General Tangent Procedure}
    \label{tab:gtp}
\end{table}

\subsection{Reverse Mode Algorithmic Differentiation}

As in \cite{dhamarticle}, consider for the image of $F$ the hyperplane $\{ y \in \mathbb{R}^m | \Bar{y}^\top y = c\}$ for a given vector $\Bar{y} \in \mathbb{R}^m$ and a given value $c \in \mathbb{R}$. The inverse image of this hyperplane is given by the set $\{ x \in \mathbb{R}^n | \Bar{y}^\top F(x) = c\}$. We can apply the implicit function theorem to this set to calculate the normal to this hyperplane at $x$ as
\begin{equation}
    \Bar{x}^\top = \Bar{y}^\top F'(x) \equiv \Bar{F}(x, \Bar{y})
\end{equation}
This calculation is provided $\Bar{x}^\top$ does not vanish.

Using the additional notation $\Bar{u}_i = (\Bar{v}_j)_{j \prec i}$, we calculate the adjoint function for each elemental function $\varphi : \mathbb{R}^{n_i} \longrightarrow \mathbb{R}$ as
\begin{equation}
    \Bar{u}_i += \Bar{v}_i \nabla \varphi_i (u_i)
\end{equation}

In reverse mode, the tree is first traversed beginning at the inputs and finishing at the outputs, storing the value the elemental function takes at each node. Then, the tree is traversed in the other direction storing the value the adjoint function takes at each node. This produces a the adjoint procedure for computing the first derivative from \cite{dhamarticle}, given below.

\begin{table}[h]
    \centering
    \begin{tabular}{|lcll|}
        \hline
        $v_{i}$ & $=$ & $0$ & $i = 1, \ldots, l$ \\
        \hline
        $[v_{i-1}, \Bar{v}_{i-1}]$ & $=$ & $[x_{i}, \Bar{x}_{i}]$ & $i = 1, \ldots, n$ \\
        \hline
        push$(v_i)$ & & & \\
        $v_{i}$ & $=$ & $v_i = \varphi_i (u_i)$ & $i = 1, \ldots, l$ \\
        \hline
        $v_{l-i}$ & $=$ & $y_{m-1}$ & $i = 1, \ldots, m-1$ \\
        $\Bar{v}_{l-i}$ & $=$ & $\bar{y}_{m-1}$ & $i = 1, \ldots, m-1$ \\
        \hline
        $v_i \leftarrow$ pop$()$ & & & \\
        $\Bar{u}_i$ & $+=$ & $\Bar{v}_i \nabla \varphi_i (u_i)$ & $i = l, \ldots, 1$ \\
        $v_i$ & $=$ & $0$ & \\
        \hline
        $\Bar{v}_{i-n}$ & $=$ & $\Bar{x}_i$ & $i = 1, \ldots, n$ \\
        \hline
    \end{tabular}
    \caption{Adjoint Procedure}
    \label{tab:ap}
\end{table}

\section{Applying Algorithmic Differentiation on DAG's}

If we consider an expression represented by a DAG, then it is very easy to apply the concept of Forward and Reverse mode algorithmic differentiation. Here we can represent each Element/Node denoted $v_i$ as our elementary function $v_i = \varphi_i (v_j)_{j \prec i}$ where now we can view $j \prec i$ to mean that $v_i$ is an operand of $v_j$. Then to apply forward mode differentiation we can compute our tangents in Equation \ref{tangentequ} by traversing this graph with post-order traversal, calculating both the values, $v_i$, and the tangent values, $\Dot{v_i}$ as we go along. So we can calculate the derivatives of all the output variables with respect to each input variable on each pass. Alternatively we can do reverse mode algorithmic differentiation by first traversing postorder traversal on the graph, computing all the values, $v_i$. Then we can preorder traverse the graph to compute the derivative of each output variable with respect to every input variable on each pass.

This gives us an insight to why forward mode has a computational cost of $\mathcal{O}(n)$. As we require to have $n$ forward passes to compute the derivative. Similary we can see that reverse mode has a computational cost of $\mathcal{O}(m)$. As we require to have $m$ reverse passes into order to calculate the derivative. Here we refer to a forward pass as the postorder traversal on a expression and a reverse pass as the preorder traversal on a expression.

For the next two sections we will be considering the equation given by
\begin{equation}
    \label{example2}
    f(x,y) = \sin(yx^2) + e^{x^2}
\end{equation}


\begin{figure}[h!]
    \includegraphics[width=4cm]{images/Graph_DAGExample.pdf}
    \caption{The DAG of Equation \ref{example2}}
    \label{fig:DAGgraph}
\end{figure}

\subsection{Example of Forward Mode Algorithmic Differentiation on a DAG}

Say we want to apply our algorithm to Equation \ref{example2} represented by the DAG in Figure \ref{fig:DAGgraph} evaluating at $x=2$, $y=2$. Here we label the nodes according to the notation used above as seen in Table \ref{example1}. 



\begin{algorithm}[h]
\caption{ForwardmodeAD algorithm}\label{forwardAD}
\begin{algorithmic}[1]
\Procedure{ForwardmodeAD}{$expression,conditions$}
\State dict = dict()\Comment{}
\For{ each $symbol$ in conditions}
    \For{ each node i in expression}\Comment{By postorder traversal, operands before element}
    \State Calculate $v_i$ and $\Dot{v_i}$ w.r.t its $\varphi_i(v_j)_{j \prec i}$ \verb|operands| and \verb|symbol|
    \State dict[symbol] = \verb|expression.adjoint|\Comment{Store the adjoint w.r.t the symbol}
    \EndFor
\EndFor
\State \textbf{return} a dictionary of symbols and their respective \verb|adjoints|
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Example of Reverse Mode Algorithmic Differentiation on a DAG}


We then traverse the graph in postorder traversal, computing the value of each node as we go along. The results after this pass are seen in Table \ref{tab:example1FP}. Now we will traverse the tree with preorder traversal. We know that first $\dot{v_6} \equiv 1$ and we have all our values calculated from the previous pass, so we get the adjoints as listed in Table \ref{tab:example1RP}


\begin{table}[h!]
    \centering
    \begin{tabular}{|lcl|lclll|}
        \hline
        $v_{-1}$ & $\equiv$ & $x$ & $\Bar{v}_{-1}$ & $=$ & $\Bar{v_2}\frac{\partial{v_2}}{\partial{v_{-1}}}$ & $=$ & $\Bar{v_2}v_1 {v_{-1}}^{(v_{1}-1)}$\\
        $v_{0}$ & $\equiv$ & $y$ & $\Bar{v}_{0}$ & $=$ & $\Bar{v_3}\frac{\partial{v_3}}{\partial{v_0}}$ & $=$ & $\Bar{v_3}v_2$\\
        \hline
        $v_{1}$ & $=$ & $2$ & $\Bar{v}_{1}$ & $=$ & $\Bar{v_2}\frac{\partial{v_2}}{\partial{v_1}}$ & $=$ & $\Bar{v}_{2}{v_{-1}}^{v_{1}}ln(v_{-1})$\\
        $v_{2}$ & $=$ & ${v_{-1}}^{v_{1}}$ & $\Bar{v}_{2}$ & $=$ & $\Bar{v_3}\frac{\partial{v_3}}{\partial{v_2}} + \Bar{v_5}\frac{\partial{v_5}}{\partial{v_2}}$ & $=$ & $\Bar{v}_{3}v_0 + \Bar{v_5}e^{v_2}$\\
        $v_{3}$ & $=$ & ${v_{0}}*{v_{2}}$ & $\Bar{v}_{3}$ & $=$ & $\Bar{v_4}\frac{\partial{v_4}}{\partial{v_3}}$ & $=$ & $\Bar{v_4}cos(v_2)$\\
        $v_{4}$ & $=$ & $sin(v_3)$ & $\Bar{v}_{4}$ & $=$ & $\Bar{v_6}\frac{\partial{v_6}}{\partial{v_4}}$ & $=$ & $\Bar{v_6}$\\
        $v_{5}$ & $=$ & $e^{v_2}$ & $\Bar{v}_{5}$ & $=$ & $\Bar{v_6}\frac{\partial{v_6}}{\partial{v_5}}$ & $=$ & $\Bar{v_6}$\\
        \hline
        $v_{6}$ & $=$ & $v_5 + v_4$ & $\Bar{v}_{6}$ & $=$ & $\frac{\partial{v_6}}{\partial{v_6}}$ & $=$ & $1$\\
        \hline
    \end{tabular}
    \caption{Table of Node Values and Adjoints in Figure \ref{fig:DAGgraph} of Equation \ref{example2}}
    \label{tab:example1}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|lclllcl|}
        \hline
        $v_{-1}$ & $=$ & $x$ & $\equiv$ & 2 & $=$ & 2\\
        $v_{0}$ & $\equiv$ & $y$ & $\equiv$ & 2 & $=$ & 2\\
        \hline
        $v_{1}$ & $\equiv$ & $2$ & $=$ & 2 & $=$ & 2\\
        $v_{2}$ & $\equiv$ & ${v_{-1}}^{v_{1}}$ & $=$ & $ 2^2$ & $=$ & $4$\\
        $v_{3}$ & $\equiv$ & $v_0 * v_2$ & $=$ & $ 2 * 4$ & $=$ & $8$\\
        $v_{4}$ & $\equiv$ & $sin(v_3)$ & $=$ & $sin(8)$ & $\approx$ & $0.9894$\\
        $v_{5}$ & $\equiv$ & $e^{v_2}$ & $=$ & $ e^4$ & $\approx$ & $54.5982$\\
        \hline
        $v_{6}$ & $\equiv$ & $v_5 + v_4$ & $=$ & $e^4 + sin(8)$ & $\approx$ & $55.5875$\\
        \hline
    \end{tabular}
    \caption{Values of Figure \ref{fig:DAGgraph} After Forward Pass}
    \label{tab:example1FP}
\end{table}


\begin{table}[h!]
    \centering
    \begin{tabular}{|lclllll|}
        \hline
        $\Bar{v}_{-1}$ & $=$ & $\Bar{v_2}v_1 {v_{-1}}^{(v_{1}-1)}$ & $=$ & $(2cos(8)+e^4) \cdot 2 \cdot 2^{2-1} = 8cos(8)+4e^4$ & $\approx$ & $217.2286$\\
        $\Bar{v}_{0}$ & $=$ & $\Bar{v_3}v_2$ & $=$ & $cos(8)\cdot4 = 4cos(8)$ & $\approx$ & $-0.5820$\\
        \hline
        $\Bar{v}_{1}$ & $=$ & $\Bar{v}_{2}{v_{-1}}^{v_{1}}ln(v_{-1})$ & $=$ & $(2cos(8)+e^4) \cdot 2^2 \cdot ln(2) = 8cos(8)ln(2) +4e^4ln(2)$ & $\approx$ & $63.1932$\\
        $\Bar{v}_{2}$ & $=$ & $\Bar{v}_{3}v_0 + \Bar{v_5}e^{v_2}$ & $=$ & $cos(8) \cdot 2 + 1 \cdot e^{4} = 2cos(8)+e^4$ & $\approx$ & $54.3071$\\
        $\Bar{v}_{3}$ & $=$ & $\Bar{v_4}cos(v_3)$ & $=$ & $1 \cdot cos(8) = cos(8)$ & $\approx$ & $-0.1455$\\
        $\Bar{v}_{4}$ & $=$ & $\Bar{v_6}$ & $=$ & $1$ & $=$ & $1$\\
        $\Bar{v}_{5}$ & $=$ & $\Bar{v_6}$ & $=$ & $1$ & $=$ & $1$\\
        \hline
        $\Bar{v}_{6}$ & $=$ & $1$ & $=$ & $1$ & $=$ & $1$\\
        \hline
    \end{tabular}
    \caption{Adjoints of Figure \ref{fig:DAGgraph} After Reverse Pass}
    \label{tab:example1RP}
\end{table}

\newpage
\section{Implementation of scalar reverse mode AD in Python}

To implement reverse mode AD in Python we used the symbolic language that we defined in \cite{PoPBook} as a template. This allowed us to have a symbolic language that we could use to easily construct our expressions. It does this by operator overloading our elementary operators so we can easily create an expression that we can traverse. Here we added additional elementary functions Sin, Cos, Exp and Log. So that they can exist in expressions we create and behave correctly. We also modified our base Expression class to contain the following attributes
\begin{verbatim}
    class Expression:
        def __init__(self, *operands):
            self.operands = operands
            self.storedvalue = 0
            self.adjoint = 0
\end{verbatim}
This allows us to store the intermediate value of our element as we traverse our DAG in our forward pass to use when calculating the adjoints in our reverse pass.
We also modified the \verb|evaluate()| function from \cite{PoPBook} to include evaluation of our functions Sin, Cos, Exp and Log. As an example here is the code for our evaluate method when our expression is Sin()
\begin{verbatim}
    @evaluate.register(expressions.Sin)
    def _(expr, *o, **kwargs):
        return np.sin(o[0])
\end{verbatim}

Then we have to create an algorithm for traversing and evaluating the DAG at each node. Note here we need to traverse the DAG in postorder traversal, where we visit the operands before we visit the element. Note in implementation we keep a dict of the expression and the stored value so we can avoid unnecessary calculation. The idea of the algorithm is below

\begin{algorithm}[h!]
\caption{EvaluatePostvisitor function}\label{EvaluatePostvisitor}
\begin{algorithmic}[1]
\Procedure{EvaluatePostvisitor}{$expression,conditions$}
\For{each element in expression}\Comment{Postorder traversal: visiting element after its operands}
\State \verb|value| $\gets$ \verb|evaluate(element, operands, conditions)|
\State \verb|element.storedvalue| $\gets$ \verb|value|
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Now that we have done a postorder traversal of our expression, each element contains a correct t\verb|element.storedvalue| attribute based on its elementary function and our evaluating conditions. Now we need to do a preorder traversal of our graph, evaluating the adjoint of each node. To do this we need to create a new \verb|adjoint_evaluate()| method similar to the previous \verb|evaluate()| method but instead it will calculate the adjoint of each of the operands with respect to the element, the Table \ref{tab:AdjEval} shows the output of \verb|adjoint_evaluate()| based on each element. Presume $w_1$ and $w_2$ are \verb|v_1.storedvalue| and \verb|v_2.storedvalue| respectively for $v_1$ and $c_2$ operands of \verb|element|. Note $\sin()$, $\cos()$, etc only have 1 operand and thus only 1 output

\begin{table}[h!]
    \centering
    \begin{tabular}{|lll|}
        \hline
        Type of \verb|element| & Operand 1  & Operand 2 \\
        \hline
        Add() & 1 & 1 \\
        Sub() & 1 & -1 \\
        Mul() & $w_2$ & $w_1$ \\
        Div() & $1/w_2$ & $\frac{-w_1}{{w_2}^2}$ \\
        Pow() & $w_2{w_1}^{(w_2-1)}$ & ${w_1}^{w_2}\log(w_1)$ \\
        Sin() & $\cos(w_1)$ &  \\
        Cos() & $-\sin(w_1)$ &  \\
        Exp() & $\exp(w_1)$ &  \\
        Log() & $\frac{1}{w_1}$ &  \\
        \hline
    \end{tabular}
    \caption{Output of \verb|adjoint\_evaluate(element)|}
    \label{tab:AdjEval}
\end{table}

Now we have our \verb|adjoint_evaluate()| method we can run this at each element when traversing the expression tree in preorder traversal, visiting the element before its operands. The general idea of the algorithm is below

\begin{algorithm}[h]
\caption{AdjointPrevisitor function}\label{AdjointPrevisitor}
\begin{algorithmic}[1]
\Procedure{AdjointPrevisitor}{$expression$, adjoint=1}
\State \verb|expression.adjoint| $\gets$ adjoint
\State \verb|adjoint| $=$ \verb|adjoint_evaluate(expression, operands)|\Comment{adjoint is a list}
\For{each index,operand in enumerate(expressions.operand)}\Comment{Preorder traversal}
\State \verb|operand.adjoint| $+=$ \verb|adjoint[index]|$\cdot$ \verb|expression.adjoint|
\State run \verb|AdjointPrevisitor(operand, operand.adjoint)|
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Now we have populated all the elements with their respective adjoints and so we can simply return a dictionary of all the symbols and their adjoint values to get the derivatives of our original expression. So now we can simply define a new function like so to calculate the derivative of an expression given some conditions

\begin{algorithm}[h]
\caption{ReversemodeAD algorithm}\label{reverseAD}
\begin{algorithmic}[1]
\Procedure{ReversemodeAD}{$expression,conditions$}
\State run \verb|EvaluatePostvisitor(expression, conditions)|
\State run \verb|AdjointPrevisitor(expression)|
\State \textbf{return} a dictionary of Symbols and their respective \verb|adjoints|
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Accuracy of results}

We can check the accuracy of our results using 2 methods, first, by using Taylor series we have the equation:
\begin{equation}
    F(x + \epsilon) = F(x) + \frac{dF}{dx} * \epsilon + O(\epsilon ^ 2)
\end{equation}
When we send $\epsilon$ to 0 the term $O(\epsilon)$ goes to 0 meaning we can find $O(\epsilon^2)$ by rearranging the equation to get:

\begin{equation}
    F(x + \epsilon) - F(x) - \frac{dF}{dx} * \epsilon
\end{equation}

When we compute this we get a value for $O(\epsilon^2)$ which as $\epsilon \to 0$ $O(\epsilon^2) \to 0$ at a rate proportional to $\epsilon^2$

\begin{center}
    \includegraphics[width=12cm]{images/Taylor_error_1.png}
\end{center}

We can see from this graph that in general as our Epsilon gets smaller so does our Taylor error, as predicted, and our Taylor error plateaus around $10^{-16}$ and this is because machine Epsilon lies in this region.

The other way we check the accuracy is by using the finite difference method, for this we use a similar method with a key difference, the formula we will use to approximate the derivative is:

\begin{equation}
    \frac{F(x + \epsilon) + F(x)}{\epsilon} - \frac{dF}{dx}
\end{equation}

As you can see here we have divided the above equation by $\epsilon$ to find $O(\epsilon^2)/\epsilon$ as we can see converges at half the rate as (a rate of $\epsilon$ instead of $\epsilon^2$):

\begin{center}
    \includegraphics[width=12cm]{images/Finite_Difference_error_1.png}
\end{center}


\section{Extention and implementation into NumPy arrays}

\section{Applications}

\bibliography{thebibliography}

%Moved the algorithm, dump sections and work sections to seperate file called roughwork.tex which we can use to put parts that are not yet going to be in the paper

\end{document}