\documentclass{article}

\usepackage{floatrow}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{parskip} % Required to stop indenting new paragraphs
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[numbers]{natbib} % number references and natbib for more styles
\bibliographystyle{unsrtnat} % sets bibliography to vancouver style

\title{Reverse Mode Algorithmic Differentiation}
\author{cf1021, sam221, aml21, mf621}
\date{}

\begin{document}

\maketitle

\section{Introduction}

When solving non-linear systems one method is Newton's method. Say the system is defined by
\begin{equation} \label{Fxy}
        F: \mathbb{R}^n \rightarrow \mathbb{R}^m \quad F(x_1, \cdots, x_n) = (y_1, \cdots, y_m)
\end{equation}
then given $x_0 \in \mathbb{R}^n$ the iterates are calculated by
\begin{equation*}
x_{n+1} = x_n - (F'(x_n))^{-1}F(x_n) \quad n \in \mathbb{N}
\end{equation*}
where $F'(x_n)$ is the Jacobian of $F$ at $x_n$ defined by
\begin{equation} \label{jacobian}
    F'(x) = \begin{bmatrix}
        \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
        \vdots & \ddots & \vdots \\
        \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
    \end{bmatrix} \in \mathbb{R}^{m \times n}
\end{equation}

Hence finding the derivative of a function is essential for us to be able to solve our system. So we need to find a way of computing the derivative of a function with low cost. Computing the derivative of the function is also essential in the development of neural networks, especially in the area of Backpropogation, a method used to train neural nets. On these neural nets, one has many nodes connected by a series of weights, such that the change in one value affects the change in another, to optimise these weights we must use a method called gradient descent which requires us to find a derivative.

From these examples and those stated in \cite{appad} we see the importance of implementation of quick derivative computation on computers.

\section{Basics of Computing Derivatives}

There are three common ways to differentiate a function on a computer: numerically, symbolically, and algorithmically. The latter which we will be discussing in depth in this paper. Notably focusing on reverse mode algorithmic differentiation (AD). In contrast to symbolic and algorithmic differentiation, numerical differentiation results in an approximation for the derivative.

The most common example of numerical differentiation is the finite differences method. For \\ $F: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ in (\ref{Fxy}), a partial derivative can be approximated using a truncated Taylor series around $p \in \mathbb{R}^n$. 
\begin{equation*}
    \frac{\partial F_i (p)}{\partial x_j} \approx \frac{F_i(p+he_j) - F_i(p)}{h}
\end{equation*}
This calculation for the derivative is approximate due to the error in the truncated Taylor series. The computational cost of calculating the Jacobian of $F$ in (\ref{jacobian}) is $n + 1$ times the cost of evaluating $F$ \cite{chem}. Thus, finite differences is unsuited for systems with a large number of unknowns.

Symbolic differentiation involves taking an algebraic expression and computing its derivative with respect to a specified variable through repeated application of basic rules of calculus using computer algebra tools. It can be useful as, once the formula for the derivative is computed, finding the value of the derivative at a point is as simple as substituting it into the formula for the derivative.  Further, symbolic differentiation provides an explicit formula for the derivative which is beneficial from a mathematical point of view. However, it has the drawback that it can only be applied when the function to be differentiated exists in a closed form. For calculating the gradient of equation \ref{Fxy}, symbolic differentiation has a computational cost of $n$ times the cost of evaluating $F$ \cite{chem}. Thus, the computational cost for calculating the Jacobian of a set of equations would be prohibitively expensive.

There are two forms of algorithmic differentiation: forward mode and reverse mode. Fundamental to AD is the application of the chain rule. Both have similar computational costs for the Jacobian but are suited to different types of functions. With forward mode algorithmic differentiation, this has computational cost proportional to $n$ times the cost of evaluating $F$. In contrast, the computational cost of $[F'(x)]^\top$ with reverse mode algorithmic differentiation is proportional to $m$ times the cost of evaluating $F$ \cite{falisse}. Hence, reverse mode is suited for functions $F$ where $n >> m$. We will see below that the reverse mode has an even better complexity error for calculating gradients compared to the alternatives. One drawback of reverse mode algorithmic differentiation over forward-mode is that it requires higher memory usage, due to the necessity to store intermediary values.

\section{Directed Acylic Graphs to Represent Expressions}

For a function $F: \mathbb{R}^3 \rightarrow \mathbb{R}^2$ defined by
\begin{equation} \label{example1}
    F \begin{pmatrix}
        x \\ y \\ z
    \end{pmatrix} = \begin{pmatrix}
        \sin (x^2 y) + e^{x^2} \\ e^{x^2} \log z
    \end{pmatrix}
\end{equation}
its representation as a DAG and an expression tree can be seen in Figure \ref{fig:DAGgraph2}.

\begin{figure}[h]
    \centering
    \includegraphics[width=12cm]{images/Graph_Cluster_1.pdf}
    \caption{A DAG and expression tree representation of Equation \ref{example1}}
    \label{fig:DAGgraph2}
\end{figure}

As in \cite{PoPBook}, we can represent an arithmetic expression as an expression tree. Here we represent each operator, symbol and number in an given expression as nodes in a graph, we denote these as elements. The operands of each element are the children of the node. We will represent links out of a node indicating the children of the node, even though expression wise the children are "inputs" to such parent. We can also write an expression as a directed acylic graph which allows us to represent expressions with more than one operators to avoid recursion. An example of the difference between the two can be see in Figure \ref{fig:DAGgraph2}. We from now on will use DAG's to represent expressions as this is essential in lowering runtime and allowing fast computation of complicated expressions. We will presume that ordering of each child is important.

\section{Fundamentals of Algorithmic Differentiation}

Using notation from \cite{evald}, let $F: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and $F(x) = y$ as in Section 2. Assume $F$ is the composition of a sequence of continuously differentiable elemental functions $(\varphi_i)_{i=1,\ldots, l}$, the derivatives of which can be easily calculated. Hence, $F$ can be represented as a DAG. In our case we will use the elemental functions provided in Table \ref{tab:elemental}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Function & Derivative \\
        \hline
        $\sin(u)$ & $\cos(u)$ \\
        $\cos(u)$ & $-\sin(u)$ \\
        $\exp(u) $ & $\exp(u)$ \\
        $\log(u)$ & $1/u$ \\
        $u^k$ & $k u^{k-1}$ \\
        $f(u)g(u)$ & $f'(u)g(u) + f(u)g'(u)$ \\
        $f(u) + g(u)$ & $f'(u) + g'(u)$ \\
        \hline
    \end{tabular}
    \caption{Derivatives of Elemental Functions}
    \label{tab:elemental}
\end{table}

The quantities calculated at each node of the graph during the function evaluation are numbered such that
\begin{equation*}
    [ \underbrace{v_{1-n}, \ldots, v_0}_{x} , v_1, \ldots, v_{l-m}, \underbrace{v_{l-m+1}, \ldots, v_l}_{y}]
\end{equation*}
Using $j \prec i$ to show $v_i$ depends directly on $v_j$ and $j \prec i \Longrightarrow j < i$ then it can be written
\begin{equation*}
    v_i = \varphi_i (v_j)_{j \prec i} \text{ for } \varphi_i : \mathbb{R}^{n_i} \longrightarrow \mathbb{R}
\end{equation*}

\subsection{Forward Mode}

We will use the interpretation of forward mode AD given in \cite{dhamarticle}. Assume $x$ is equal to the value a time-dependent $x(t)$ takes at $t=0$. Then define the tangent 
\begin{equation*}
    \dot{x} = \frac{\partial x}{\partial t} \Big|_{t=0}
\end{equation*}
As $y = F(x)$ we calculate the tangent $\dot{y}$ as
\begin{equation*}
    \Dot{y} = \frac{\partial}{\partial t} F(x (t)) \Big|_{t=0} 
    = F'(x (t)) \frac{\partial x}{\partial t} \Big|_{t=0}
    = F'(x) \Dot{x}
    \equiv \Dot{F}(x, \Dot{x})
\end{equation*}
Using the additional notation $u_i = (v_j)_{j \prec i}$ and $\Dot{u}_i = (\Dot{v}_j)_{j \prec i}$, the idea can be applied to an elemental function $\varphi_i : \mathbb{R}^{n_i} \longrightarrow \mathbb{R}$, $v_i = \varphi_i (u_i)$ to get the result
\begin{equation*}
    \label{tangentequ}
    \Dot{v_i} = \sum_{j \prec i} \left\{ \Dot{v}_j \cdot \frac{\partial}{\partial v_j} \varphi_i (u_i) \right\} 
    \equiv \Dot{\varphi}_i(u_i, \Dot{u}_i)
\end{equation*}

This can be summarised as the general evaluation procedure given in \cite{evald} and given in Table \ref{tab:gep}.

\begin{table}[h]
    \centering
    \begin{tabular}{|lcll|}
        \hline
        $v_{i-n}$ & $\equiv$ & $x_i$ & $i = 1, \ldots, n$ \\
        $\Dot{v}_{i-n}$ & $\equiv$ & $\Dot{x}_i$ & $i = 1, \ldots, n$ \\
        \hline
        $v_{i}$ & $\equiv$ & $v_i = \varphi_i (u_i)$ & $i = 1, \ldots, l$ \\
        $\Dot{v}_{i}$ & $\equiv$ & $\sum_{j \prec i} \left\{ \Dot{v}_j \cdot \frac{\partial}{\partial v_j} \varphi_i (u_i) \right\}$ & $i = 1, \ldots, l$ \\
        \hline
        $v_{l-m+i}$ & $\equiv$ & $y_i$ & $i = 1, \ldots, m$ \\
        $\Dot{v}_{l-m+i}$ & $\equiv$ & $\Dot{y}_i$ & $i = 1, \ldots, m$ \\
        \hline
    \end{tabular}
    \caption{General Evaluation Procedure}
    \label{tab:gep}
\end{table}

When represented on a DAG, forward mode AD begins at the inputs and moves through the graph computing $F$ and $\Dot{F}$ together at each node. For this a concise version of Table \ref{tab:gep} is provided in Table \ref{tab:gtp}.

\begin{table}[h]
    \centering
    \begin{tabular}{|lcll|}
        \hline
        $[v_{i-n}, \Dot{v}_{i-n}]$ & $=$ & $[x_{i}, \Dot{x}_{i}]$ & $i = 1, \ldots, n$ \\
        \hline
        $[v_{i}, \Dot{v}_{i}]$ & $=$ & $[\varphi_i (u_i), \Dot{\varphi}_i(u_i, \Dot{u}_i)]$ & $i = 1, \ldots, l$ \\
        \hline
        $[v_{l-m+1}, \Dot{v}_{l-m+1}]$ & $=$ & $[y_{i}, \Dot{y}_{i}]$ & $i = 1, \ldots, m$ \\
        \hline
    \end{tabular}
    \caption{General Tangent Procedure}
    \label{tab:gtp}
\end{table}

\subsection{Reverse Mode}

As in \cite{dhamarticle}, consider for the image of $F$ the hyperplane $\{ y \in \mathbb{R}^m | \Bar{y}^\top y = c\}$ for a given vector $\Bar{y} \in \mathbb{R}^m$ and a given value $c \in \mathbb{R}$. The inverse image of this hyperplane is given by the set $\{ x \in \mathbb{R}^n | \Bar{y}^\top F(x) = c\}$. We can apply the implicit function theorem to this set to calculate the normal to this hyperplane at $x$ as
\begin{equation*}
    \Bar{x}^\top = \Bar{y}^\top F'(x) \equiv \Bar{F}(x, \Bar{y})
\end{equation*}
This calculation is provided $\Bar{x}^\top$ does not vanish.

Using the additional notation $\Bar{u}_i = (\Bar{v}_j)_{j \prec i}$, we calculate the adjoint function for each elemental function $\varphi : \mathbb{R}^{n_i} \longrightarrow \mathbb{R}$ as
\begin{equation*}
    \Bar{u}_i += \Bar{v}_i \nabla \varphi_i (u_i)
\end{equation*}

In reverse mode, the tree is first traversed beginning at the inputs and finishing at the outputs, storing the value the elemental function takes at each node. Then, the tree is traversed in the other direction storing the value the adjoint function takes at each node. This produces a the adjoint procedure for computing the first derivative from \cite{dhamarticle}, given in Table \ref{tab:ap}.

\begin{table}[h]
    \centering
    \begin{tabular}{|llll|}
        \hline
        $v_{i}$ & $=$ & $0$ & $i = 1, \ldots, l$ \\
        \hline
        $[v_{i-1}, \Bar{v}_{i-1}]$ & $=$ & $[x_{i}, \Bar{x}_{i}]$ & $i = 1, \ldots, n$ \\
        \hline
        push$(v_i)$ & & & \\
        $v_{i}$ & $=$ & $v_i = \varphi_i (u_i)$ & $i = 1, \ldots, l$ \\
        \hline
        $v_{l-i}$ & $=$ & $y_{m-1}$ & $i = 1, \ldots, m-1$ \\
        $\Bar{v}_{l-i}$ & $=$ & $\bar{y}_{m-1}$ & $i = 1, \ldots, m-1$ \\
        \hline
        $v_i \leftarrow$ pop$()$ & & & \\
        $\Bar{u}_i$ & $+=$ & $\Bar{v}_i \nabla \varphi_i (u_i)$ & $i = l, \ldots, 1$ \\
        $v_i$ & $=$ & $0$ & \\
        \hline
        $\Bar{v}_{i-n}$ & $=$ & $\Bar{x}_i$ & $i = 1, \ldots, n$ \\
        \hline
    \end{tabular}
    \caption{Adjoint Procedure}
    \label{tab:ap}
\end{table}

\section{Complexity Results for Algorithmic Differentiation}

Below we will use notation and results from \cite{dhamarticle} for the number of operations and the memory usage required to compute a function. The number of operations is denoted $\text{OPS}(\cdot)$. The amount of random access memory and the amount of sequentially accessed memory are denoted $\text{RMEM}(\cdot)$ and $\text{SMEM}(\cdot)$ respectively. 

\subsection{Forward Mode}

Given $x, \Dot{x} \in \mathbb{R}^n$ we attain an upper bounded for calculating the number of operations required to compute $F'(x) \Dot{x}$ given in (\ref{ops1}). Given an upper bound on the time taken to compute one operation calculation on the bounded for the time taken to compute $F'(x) \Dot{x}$ is simple.
\begin{equation} \label{ops1}
    \text{OPS}(F'(x) \Dot{x}) \leq 3 \cdot \text{OPS}(F(x))
\end{equation}
Further, we get an exact value for the memory requirement in (\ref{rmem1}).
\begin{equation} \label{rmem1}
    \text{RMEM}(F'(x) \Dot{x}) = 2 \cdot \text{RMEM}(F(x))
\end{equation}

\subsection{Reverse Mode}

Given $x \in \mathbb{R}^n$ and $\Bar{y} \in \mathbb{R}^m$ we attain an upper bound for calculating the number of operations required to compute $\Bar{y}^\top F'(x)$ given in (\ref{ops2}). Similarly to before, an upper bound for the time taken to compute $\Bar{y}^\top F'(x)$ can be calculated provided we know the time taken to compute one operation.
\begin{equation} \label{ops2}
    \text{OPS}(\Bar{y} F'(x)) \leq 4 \cdot \text{OPS}(F(x))
\end{equation}
The memory usage is given in (\ref{rmem2}). Here we see the random access memory usage is the same as forward mode but the use of sequentially accessed memory creates an overall memory usage increase.
\begin{equation} \label{rmem2}
    \begin{tabular}{c}
        $\text{RMEM}(\Bar{y} F'(x)) = 2 \cdot \text{RMEM}(F(x))$ \\ 
        $\text{SMEM}(\Bar{y} F'(x)) \approx \text{OPS}(F(x))$
    \end{tabular}
\end{equation}

\section{Applying Algorithmic Differentiation on Directed Acyclic Graphs}

If we consider an expression represented by a DAG, then it is very easy to apply the concept of Forward and Reverse mode algorithmic differentiation. Here we can represent each Element/Node denoted $v_i$ as our elementary function $v_i = \varphi_i (v_j)_{j \prec i}$ where now we can view $j \prec i$ to mean that $v_i$ is an operand of $v_j$. Then to apply forward mode differentiation we can compute our tangents in (\ref{tangentequ}) by traversing this graph with post-order traversal, calculating both the values, $v_i$, and the tangent values, $\Dot{v_i}$ as we go along. So we can calculate the derivatives of all the output variables with respect to each input variable on each pass. Alternatively we can do reverse mode algorithmic differentiation by first traversing postorder traversal on the graph, computing all the values, $v_i$. Then we can preorder traverse the graph to compute the derivative of each output variable with respect to every input variable on each pass.

This gives us an idea of why forward mode has a computational cost proportional to $n$. As we require to have $n$ forward passes to compute the derivative. Similary we can see that reverse mode has a computational cost proportional to $m$. As we require to have $m$ reverse passes into order to calculate the derivative. Here we refer to a forward pass as the postorder traversal on a expression and a reverse pass as the preorder traversal on a expression.

For the next two sections we will be considering the equation given by
\begin{equation}
    \label{example2}
    f(x,y) = \sin(yx^2) + e^{x^2}
\end{equation}


\begin{figure}[h!]
    \includegraphics[width=4cm]{images/Graph_Example2.pdf}
    \caption{DAG of (\ref{example2})}
    \label{fig:DAGgraph}
\end{figure}

\subsection{Example of Forward Mode}

Say we want to apply our algorithm to (\ref{example2}) represented by the DAG in Figure \ref{fig:DAGgraph} evaluating at $x=2$, $y=2$. Here we label the nodes according to the notation used above as seen in Table \ref{example1}. 



\begin{algorithm}[h]
\caption{ForwardmodeAD algorithm}\label{forwardAD}
\begin{algorithmic}[1]
\Procedure{ForwardmodeAD}{$expression,conditions$}
\State dict = dict()\Comment{}
\For{ each $symbol$ in conditions}
    \For{ each node i in expression}\Comment{By postorder traversal, operands before element}
    \State Calculate $v_i$ and $\Dot{v_i}$ w.r.t its $\varphi_i(v_j)_{j \prec i}$ \verb|operands| and \verb|symbol|
    \State dict[symbol] = \verb|expression.adjoint|\Comment{Store the adjoint w.r.t the symbol}
    \EndFor
\EndFor
\State \textbf{return} a dictionary of symbols and their respective \verb|adjoints|
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Example of Reverse Mode}


We then traverse the graph in postorder traversal, computing the value of each node as we go along. The results after this pass are seen in Table \ref{tab:example1FP}. Now we will traverse the tree with preorder traversal. We know that first $\dot{v_6} \equiv 1$ and we have all our values calculated from the previous pass, so we get the adjoints as listed in Table \ref{tab:example1RP}


\begin{table}[h!]
    \centering
    \begin{tabular}{|lcl|lclll|}
        \hline
        $v_{-1}$ & $\equiv$ & $x$ & $\Bar{v}_{-1}$ & $=$ & $\Bar{v_2}\frac{\partial{v_2}}{\partial{v_{-1}}}$ & $=$ & $\Bar{v_2}v_1 {v_{-1}}^{(v_{1}-1)}$\\
        $v_{0}$ & $\equiv$ & $y$ & $\Bar{v}_{0}$ & $=$ & $\Bar{v_3}\frac{\partial{v_3}}{\partial{v_0}}$ & $=$ & $\Bar{v_3}v_2$\\
        \hline
        $v_{1}$ & $=$ & $2$ & $\Bar{v}_{1}$ & $=$ & $\Bar{v_2}\frac{\partial{v_2}}{\partial{v_1}}$ & $=$ & $\Bar{v}_{2}{v_{-1}}^{v_{1}}ln(v_{-1})$\\
        $v_{2}$ & $=$ & ${v_{-1}}^{v_{1}}$ & $\Bar{v}_{2}$ & $=$ & $\Bar{v_3}\frac{\partial{v_3}}{\partial{v_2}} + \Bar{v_5}\frac{\partial{v_5}}{\partial{v_2}}$ & $=$ & $\Bar{v}_{3}v_0 + \Bar{v_5}e^{v_2}$\\
        $v_{3}$ & $=$ & ${v_{0}}*{v_{2}}$ & $\Bar{v}_{3}$ & $=$ & $\Bar{v_4}\frac{\partial{v_4}}{\partial{v_3}}$ & $=$ & $\Bar{v_4}cos(v_2)$\\
        $v_{4}$ & $=$ & $sin(v_3)$ & $\Bar{v}_{4}$ & $=$ & $\Bar{v_6}\frac{\partial{v_6}}{\partial{v_4}}$ & $=$ & $\Bar{v_6}$\\
        $v_{5}$ & $=$ & $e^{v_2}$ & $\Bar{v}_{5}$ & $=$ & $\Bar{v_6}\frac{\partial{v_6}}{\partial{v_5}}$ & $=$ & $\Bar{v_6}$\\
        \hline
        $v_{6}$ & $=$ & $v_5 + v_4$ & $\Bar{v}_{6}$ & $=$ & $\frac{\partial{v_6}}{\partial{v_6}}$ & $=$ & $1$\\
        \hline
    \end{tabular}
    \caption{Table of Node Values and Adjoints in Figure \ref{fig:DAGgraph} of (\ref{example2})}
    \label{tab:example1}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{|lclllcl|}
        \hline
        $v_{-1}$ & $=$ & $x$ & $\equiv$ & 2 & $=$ & 2\\
        $v_{0}$ & $\equiv$ & $y$ & $\equiv$ & 2 & $=$ & 2\\
        \hline
        $v_{1}$ & $\equiv$ & $2$ & $=$ & 2 & $=$ & 2\\
        $v_{2}$ & $\equiv$ & ${v_{-1}}^{v_{1}}$ & $=$ & $ 2^2$ & $=$ & $4$\\
        $v_{3}$ & $\equiv$ & $v_0 * v_2$ & $=$ & $ 2 * 4$ & $=$ & $8$\\
        $v_{4}$ & $\equiv$ & $sin(v_3)$ & $=$ & $sin(8)$ & $\approx$ & $0.9894$\\
        $v_{5}$ & $\equiv$ & $e^{v_2}$ & $=$ & $ e^4$ & $\approx$ & $54.5982$\\
        \hline
        $v_{6}$ & $\equiv$ & $v_5 + v_4$ & $=$ & $e^4 + sin(8)$ & $\approx$ & $55.5875$\\
        \hline
    \end{tabular}
    \caption{Values of Figure \ref{fig:DAGgraph} After Forward Pass}
    \label{tab:example1FP}
\end{table}


\begin{table}[h!]
    \centering
    \begin{tabular}{|lclllll|}
        \hline
        $\Bar{v}_{-1}$ & $=$ & $\Bar{v_2}v_1 {v_{-1}}^{(v_{1}-1)}$ & $=$ & $(2cos(8)+e^4) \cdot 2 \cdot 2^{2-1} = 8cos(8)+4e^4$ & $\approx$ & $217.2286$\\
        $\Bar{v}_{0}$ & $=$ & $\Bar{v_3}v_2$ & $=$ & $cos(8)\cdot4 = 4cos(8)$ & $\approx$ & $-0.5820$\\
        \hline
        $\Bar{v}_{1}$ & $=$ & $\Bar{v}_{2}{v_{-1}}^{v_{1}}ln(v_{-1})$ & $=$ & $(2cos(8)+e^4) \cdot 2^2 \cdot ln(2) = 8cos(8)ln(2) +4e^4ln(2)$ & $\approx$ & $63.1932$\\
        $\Bar{v}_{2}$ & $=$ & $\Bar{v}_{3}v_0 + \Bar{v_5}e^{v_2}$ & $=$ & $cos(8) \cdot 2 + 1 \cdot e^{4} = 2cos(8)+e^4$ & $\approx$ & $54.3071$\\
        $\Bar{v}_{3}$ & $=$ & $\Bar{v_4}cos(v_3)$ & $=$ & $1 \cdot cos(8) = cos(8)$ & $\approx$ & $-0.1455$\\
        $\Bar{v}_{4}$ & $=$ & $\Bar{v_6}$ & $=$ & $1$ & $=$ & $1$\\
        $\Bar{v}_{5}$ & $=$ & $\Bar{v_6}$ & $=$ & $1$ & $=$ & $1$\\
        \hline
        $\Bar{v}_{6}$ & $=$ & $1$ & $=$ & $1$ & $=$ & $1$\\
        \hline
    \end{tabular}
    \caption{Adjoints of Figure \ref{fig:DAGgraph} After Reverse Pass}
    \label{tab:example1RP}
\end{table}

\newpage
\section{Implementation of Scalar Reverse Mode in Python}

To implement reverse mode AD in Python we used the symbolic language that we defined in \cite{PoPBook} as a template. This allowed us to have a symbolic language that we could use to easily construct our expressions. It does this by operator overloading our elementary operators so we can easily create an expression that we can traverse. Here we added additional elementary functions Sin, Cos, Exp and Log. So that they can exist in expressions we create and behave correctly. We also modified our base Expression class to contain the following attributes
\begin{verbatim}
    class Expression:
        def __init__(self, *operands):
            self.operands = operands
            self.storedvalue = 0
            self.adjoint = 0
\end{verbatim}
This allows us to store the intermediate value of our element as we traverse our DAG in our forward pass to use when calculating the adjoints in our reverse pass.
We also modified the \verb|evaluate()| function from \cite{PoPBook} to include evaluation of our functions Sin, Cos, Exp and Log. As an example here is the code for our evaluate method when our expression is Sin()
\begin{verbatim}
    @evaluate.register(expressions.Sin)
    def _(expr, *o, **kwargs):
        return np.sin(o[0])
\end{verbatim}

Then we have to create an algorithm for traversing and evaluating the DAG at each node. Note here we need to traverse the DAG in postorder traversal, where we visit the operands before we visit the element. Note in implementation we keep a dictionary of the expression and the stored value so we can avoid unnecessary calculation and to make the traversal non-recursive. The idea of the algorithm is seen in algorithm \ref{EvaluatePostvisitor}

\begin{algorithm}[h!]
\caption{EvaluatePostvisitor function}\label{EvaluatePostvisitor}
\begin{algorithmic}[1]
\Procedure{EvaluatePostvisitor}{$expression,conditions$}
\For{each element in expression}\Comment{Postorder traversal: visiting element after its operands}
\State \verb|value| $\gets$ \verb|evaluate(element, operands, conditions)|
\State \verb|element.storedvalue| $\gets$ \verb|value|
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Now that we have done a postorder traversal of our expression, each element contains a correct t\verb|element.storedvalue| attribute based on its elementary function and our evaluating conditions. Now we need to do a preorder traversal of our graph, evaluating the adjoint of each node. To do this we need to create a new \verb|adjoint_evaluate()| method similar to the previous \verb|evaluate()| method but instead it will calculate the adjoint of each of the operands with respect to the element, the Table \ref{tab:AdjEval} shows the output of \verb|adjoint_evaluate()| based on each element. Presume $w_1$ and $w_2$ are \verb|v_1.storedvalue| and \verb|v_2.storedvalue| respectively for $v_1$ and $c_2$ operands of \verb|element|. Note $\sin()$, $\cos()$, etc only have 1 operand and thus only 1 output

\begin{table}[h!]
    \centering
    \begin{tabular}{|lll|}
        \hline
        Type of \verb|element| & Operand 1  & Operand 2 \\
        \hline
        Add() & 1 & 1 \\
        Sub() & 1 & -1 \\
        Mul() & $w_2$ & $w_1$ \\
        Div() & $1/w_2$ & $\frac{-w_1}{{w_2}^2}$ \\
        Pow() & $w_2{w_1}^{(w_2-1)}$ & ${w_1}^{w_2}\log(w_1)$ \\
        Sin() & $\cos(w_1)$ &  \\
        Cos() & $-\sin(w_1)$ &  \\
        Exp() & $\exp(w_1)$ &  \\
        Log() & $\frac{1}{w_1}$ &  \\
        \hline
    \end{tabular}
    \caption{Output of \verb|adjoint\_evaluate(element)|}
    \label{tab:AdjEval}
\end{table}

Now we have our \verb|adjoint_evaluate()| method we can run this function at each element when traversing the expression tree in preorder traversal, visiting the element before its operands. The general idea of the algorithm is described in algorithm \ref{AdjointPrevisitor}

\begin{algorithm}[h]
\caption{AdjointPrevisitor function}\label{AdjointPrevisitor}
\begin{algorithmic}[1]
\Procedure{AdjointPrevisitor}{$expression$, adjoint=1}
\State \verb|expression.adjoint| $\gets$ adjoint
\State \verb|adjoint| $=$ \verb|adjoint_evaluate(expression, operands)|\Comment{adjoint is a list}
\For{each index,operand in enumerate(expressions.operand)}\Comment{Preorder traversal}
\State \verb|operand.adjoint| $+=$ \verb|adjoint[index]|$\cdot$ \verb|expression.adjoint|
\State run \verb|AdjointPrevisitor(operand, operand.adjoint)|
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Now we have populated all the elements with their respective adjoints and so we can simply return a dictionary of all the symbols and their adjoint values to get the derivatives of our original expression. So now we can simply define a new function to calculate the derivative of an expression given some conditions. The general idea of this function is seen in Algorithm \ref{reverseAD}

\begin{algorithm}[h]
\caption{ReversemodeAD algorithm}\label{reverseAD}
\begin{algorithmic}[1]
\Procedure{ReversemodeAD}{$expression,conditions$}
\State run \verb|EvaluatePostvisitor(expression, conditions)|
\State run \verb|AdjointPrevisitor(expression)|
\State \textbf{return} a dictionary of Symbols and their respective \verb|adjoints|
\EndProcedure
\end{algorithmic}
\end{algorithm}

Now we can test our implementation on the previous example given in Equation \ref{example2} represented by the DAG in Figure \ref{fig:DAGgraph}. To do this we run the code below. We see our output is exactly the same as we calculated previously.

\begin{verbatim}
    sin = expressions.Sin()
    exp = expressions.Exp()
    x = expressions.Symbol('x')
    y = expressions.Symbol('y')
    expression = sin(y * x**2) + exp(x**2)
    conditions = {x:2, y:2}
    reversemodeAD(expression, conditions)
    > {'x': 217.22859986210804, 'y': -0.5820001352344542}
\end{verbatim}



\section{Accuracy of Results}

We can check the accuracy of our results using 2 methods, first, by using Taylor series we have the equation:
\begin{equation}
    F(x + \epsilon) = F(x) + \frac{dF}{dx} * \epsilon + O(\epsilon ^ 2)
\end{equation}
When we send $\epsilon$ to 0 the term $O(\epsilon)$ goes to 0 meaning we can find $O(\epsilon^2)$ by rearranging the equation to get:

\begin{equation}
    F(x + \epsilon) - F(x) - \frac{dF}{dx} * \epsilon
\end{equation}

When we compute this we get a value for $O(\epsilon^2)$ which as $\epsilon \to 0$ $O(\epsilon^2) \to 0$ at a rate proportional to $\epsilon^2$

\begin{center}
    \includegraphics[width=12cm]{images/Taylor_error_1.png}
\end{center}

When checking if our result is correct we need to calculate the rate of convergence, since we are finding $O(\epsilon^2)$ we expect our convergence to be at the rate of $\epsilon^2$. To check our rate of convergence we use the formula:

\begin{equation}
    \frac{\ln{\frac{\|u_1 - u\|}{\|u_2 - u\|}}}{\ln{\frac{|h_1|}{|h_2|}}} = \frac{\ln\|u_1 - u\| - \ln\|u_2 - u\|}{\ln|h_1| - |h_2|}
\end{equation}

Where $u_1, u_2$ are our approximations using $F(x + \epsilon) - F(x)$, $u$ is our calculated derivative using Reverse Mode AD, and $h_1, h_2$ are the corresponding $\epsilon$ for $u_1, u_2$. This value should give us the power of $\epsilon$ to which we are converging at. In our case we got: \input{images/rate_of_convergence.txt} which is significantly close enough to 2 to tell us we are converging at a rate of $\epsilon^2$.





\section{Extension and Implementation into NumPy Arrays}

Now we consider our expression as being a function of $F: \mathbb{R}^n \longrightarrow \mathbb{R}^m$. We cannot do this with our current implementation. Instead, we need to represent our expression as a NumPy 1 x m array, where each element of our array consists of the corresponding output. We can now change our algorithms for evaluation of the value and adjoint so they work correctly, this involves simply running our reversemode algorithm for each subexpression in our array, and making sure to record the adjoints of each of the symbols at each subexpresion. The modified algorithms are seen in Algorithm \ref{reverseADArr} and \ref{EvaluatePostvisitorArr}. Noting that our postvisitor traversal we use is still non-recursive.

\begin{algorithm}[h]
\caption{ReversemodeAD algorithm for arrays}\label{reverseADArr}
\begin{algorithmic}[1]
\Procedure{ReversemodeAD}{$expression,conditions$}
\State run \verb|EvaluatePostvisitor(expression, conditions)|
\For{each subexpression in expression}
\State run \verb|AdjointPrevisitor(subexpression)|
\EndFor
\State \textbf{return} a dictionary of Symbols and their respective \verb|adjoints| for each subexpression
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
\caption{EvaluatePostvisitor algorithm for arrays}\label{EvaluatePostvisitorArr}
\begin{algorithmic}[1]
\Procedure{EvaluatePostvisitor}{$expression,conditions$}
\For{each subexpression in expression}
\For{each element in subexpression}\Comment{Postorder non-recursive traversal}
\State \verb|value| $\gets$ \verb|evaluate(element, operands, conditions)|
\State \verb|element.storedvalue| $\gets$ \verb|value|
\EndFor
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

Now we can compute the adjoints of our DAG in Figure \ref{fig:DAGgraph2} for Equation \ref{example1} using our modified code. Here we get the following result for initial conditions $x=1$, $y=\pi$, $z=1$:

\begin{verbatim}
    sin = expressions.Sin()
    exp = expressions.Exp()
    log = expressions.Log()
    x = expressions.Symbol('x')
    y = expressions.Symbol('y')
    z = expressions.Symbol('z')
    expression = np.array([log(z) * exp(x**2), exp(x**2) + sin(x**2 * y)])
    conditions = {x:1, y:np.pi, z:1}
    reversemodeAD(expression, conditions)
    > {'x': [0.0, -0.846621650261496], 'y': [0, -1.0], 'z': [2.718281828459045, 0]}
\end{verbatim}

Which is what we would expect.




\section{Difference in Runtime}

Lets say we want to plot the time taken to compute various functions of $n$ unique input variables, as seen in Equation \ref{TimeFunc}. We can compare the difference in runtime between our Forward Mode AD and Reverse Mode AD. To give us a more robust plot we compute 1000 different derivatives at each stage. The results of this test is seen in Figure \ref{fig:TimeDiff}
\begin{equation} 
    \label{TimeFunc}
    \begin{split}
        f_1(\textbf{x}) = \prod_{i=1}^n x_i & \quad f_2(\textbf{x}) = \prod_{i=1}^n \sin(x_i) \\
        f_3(\textbf{x}) = \prod_{i=1}^n \exp(x_i) & \quad f_4(\textbf{x}) = \prod_{i=1}^n \log(x_i)
    \end{split}
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=15cm]{images/Graph_TimeDiff.pdf}
    \caption{Time taken to compute 1000 derivatives of the functions in Equation \ref{TimeFunc} varying the number of input parameters}
    \label{fig:TimeDiff}
\end{figure}

As expected, Forward Mode automatic differentiation grows around linearly on the number of unique input variables that we have. This further proves the importance of using Reverse Mode automatic differentiation when working with systems with large amounts of variables like neural networks.



\section{Applications}

\subsection{Advection Diffusion Equation}

Consider the simplified 1 dimensional advection-diffusion equation given by Equation \ref{AdvectionDiffusion} where we are measuring the concentration of some liquid over time where we also account for velocity and diffusion.

\begin{equation}
    \label{AdvectionDiffusion}
    \frac{\partial C}{\partial t} = - V\frac{\partial C}{\partial x} + D\frac{\partial^2 C}{\partial x^2}
\end{equation}



\bibliography{thebibliography}

\end{document}